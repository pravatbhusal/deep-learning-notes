{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks (ANNs)\n",
    "The fundamental neural network to deep learning. We will create a simple Dense ANN using Keras in this lecture, but first let's learn about the essential mechanisms of an artificial neural network.\n",
    "\n",
    "### Non-Neural Network Learning\n",
    "When not using a neural network, then you just using a bunch of if statements to check if certain conditions are true then it helps determine a predicted value.\n",
    "\n",
    "For instance, classifying an animal as a dog or cat. We would use if statements to check if the animal's ears are pointy or lopped, snout is small or large, etc.\n",
    "\n",
    "### Neural Network Learning\n",
    "When using a neural network, you program the network's architecture then it'll determine the necessary characteristics itself.\n",
    "\n",
    "For instance, the neural network would learn the difference between a dog or cat and use what it learned to predict whether or not an animal is a dog or cat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neurons and Layers\n",
    "A neuron is a Node that transmits data through layers.  \n",
    "A layer is a collection of Nodes together at the same depth.\n",
    "\n",
    "<img src=\"images/ann/neuron.png\" height=\"65%\" width=\"65%\"></img>\n",
    "- This is an example of a Perceptron: a neural network with only an input and output layer\n",
    "\n",
    "The input value(s) are standarized (and sometimes normalized).  \n",
    "The output value(s) can be continuous (number), binary (yes or no), or categorical (dummy variables).\n",
    "\n",
    "The input value(s) are transmitted through neuron(s), which are processed throughout the layers to determine output value(s).\n",
    "\n",
    "In real life, the input values of a human are the 5 senses: sight, touch, sound, taste, and smell. And these input values are transmitted to the neurons, which the neurons will process and determine an output. For example, if I touch a fire with my hand, then my touch input value will signal the neuron, and then the neuron will process it and determine that I need to take my hand away from the fire.\n",
    "\n",
    "### Weights\n",
    "For each synapse (signal), there can be weights to measure the significance of a signal. Weights are crucial, and they're the values that get adjusted across the neural network. This is where gradient descent and backpropagation come into play, but we'll get to that later.\n",
    "\n",
    "<img src=\"images/ann/weights_neuron.png\" height=\"75%\" width=\"75%\"></img>\n",
    "- W1, W2, and Wm are the individual weights for each synapse (arrow, or signal)\n",
    "\n",
    "The formula inside the neuron is a value that determines how significant the synapse is to the next neuron in the layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function\n",
    "The activation function is a function that normalizes the weighted sum of the input values. There are many types of activation functions, and some work better than others depending on the neural network.\n",
    "\n",
    "The input values must be featured scaled (standarized or normalized) for these functions to work properly.\n",
    "\n",
    "Below are different types of activation functions.\n",
    "- The x-value is the weighted sum of the input value(s)\n",
    "- The y-value is the neuron's contribution to the output value(s)\n",
    "\n",
    "### 1. Threshold Function\n",
    "A \"binary\" (yes or no) activation function.\n",
    "\n",
    "<img src=\"images/ann/threshold_function.png\" height=\"50%\" width=\"50%\"></img>\n",
    "\n",
    "### 2. Sigmoid Function\n",
    "A smooth activation function with gradual progression, very useful for the output layer to predict the probability of success.\n",
    "\n",
    "<img src=\"images/ann/sigmoid_function.png\" height=\"50%\" width=\"50%\"></img>\n",
    "\n",
    "### 3. Rectifier Function\n",
    "One of the most popular functions, a linear curve that increases after the x-value of 0.\n",
    "\n",
    "<img src=\"images/ann/rectifier_function.png\" height=\"50%\" width=\"50%\"></img>\n",
    "\n",
    "### 4. Hyperbolic Tangent (tanh)\n",
    "Similar to the sigmoid function, but the function's value can be a negative.\n",
    "\n",
    "<img src=\"images/ann/tanh_function.png\" height=\"50%\" width=\"50%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Do Neural Networks Work?\n",
    "Let's learn how neural networks actually work.\n",
    "\n",
    "### Shallow Neural Network\n",
    "In machine learning algorithms without deep learning, the algorithm can be modelled below.\n",
    "\n",
    "<img src=\"images/ann/basic_neural_network.png\" height=\"50%\" width=\"50%\"></img>\n",
    "\n",
    "This neural network is very basic: there are only independent variables (input layer), parameter tuning variables (weights), and a dependent variable (output layer). This is actually how most machine learning models work if there is no deep learning involved.\n",
    "\n",
    "Fortunately, in deep learning, there are \"hidden\" layers that increase the accuracy of the model. A \"hidden\" layer is any layer in-between the input and output layers of the neural network.\n",
    "\n",
    "### Deep Neural Network\n",
    "A deep neural network has \"hidden\" layers that process the input values further. Let's assume a neural network has already been trained, so let's observe how it will work.\n",
    "\n",
    "The neural network below is trying to predict the price of a house based on area, bedrooms, distance to city, and age.\n",
    "\n",
    "<img src=\"images/ann/neural_network_house_price.png\" height=\"50%\" width=\"50%\"></img>\n",
    "\n",
    "Each neuron in the hidden layer only accepts only some input values because of the weights from the synapses (signals) to calculate whether or not a signal is significant enough for the neuron.\n",
    "\n",
    "For example, the middle neuron in the hidden layer focuses on only the \"Area\", \"Bedrooms\", and \"Age\" input values. Maybe because the already trained neuron determined that younger people prefer high area and lots of bedrooms, so only those neurons are significant enough to impact the middle neuron.\n",
    "\n",
    "Another example is the last neuron in the hidden layer that focuses on only the \"Age\". Maybe because the neuron determined that a house older than 100+ years is priced significantly higher due to historical reasons. This is a good example of when to use the rectifier activiation function because the neuron would check if the age is 100+ then the neuron's contribution to the output increases and if not then the neuron's contribution to the output is 0.\n",
    "\n",
    "Together, all the neurons can be used to predict the price of a house as seen in the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Propagations\n",
    "There are two types of propagations: front and back propagation. These propagations are necessary in order for the neural network to learn the trends of the data set.\n",
    "\n",
    "Let's say we're trying to determine a person's exam score based on hos or her study hours, sleep hours, and quiz score. \n",
    "\n",
    "### Forward Propagation\n",
    "<img src=\"images/ann/forward_propagation.png\" height=\"50%\" width=\"50%\"></img>\n",
    "- The output (predicted) exam score is noted as y^ and the actual exam score is noted as y\n",
    "\n",
    "In forward propagation, the neural network predicts an output (predicted) value. The neural network uses a Cost function (the Mean-Squared Error function) to compare the predicted to the actual value.\n",
    "\n",
    "### Back Propagation\n",
    "<img src=\"images/ann/back_propagation.png\" height=\"50%\" width=\"50%\"></img>\n",
    "\n",
    "Then using the cost function, the network signals a back propagation to update the weights of the synapses.\n",
    "\n",
    "### Epoch\n",
    "One epoch is when the entire data set is forward and back propgagated. The goal is to minimize the cost function, so we must perform multiple epochs to better learn the trends of the data set.\n",
    "\n",
    "However, too many epochs may cause overfitting of the data set. It means that your model does not learn the data, it memorizes the data. To avoid overfitting, early stop the model once the validation accuracy flattens out, or it starts decreasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "Now that we understand that back propagation sends a signal back to the neurons to update the synapse weights, we also need to understand how the weights are actually adjusted.\n",
    "\n",
    "The goal is to minimize the cost function, so which weight values could accomplish that?\n",
    "\n",
    "### Brute Force Approach\n",
    "If we decided to brute force and guess the weights and there are too many input values, then it would be inefficient because there's too many combinations to compute (curse of dimensionality).\n",
    "\n",
    "### Gradient Descent Approach\n",
    "Graph the cost function where C is the dependent variable, y^ is the independent variable, and y is the vertex.\n",
    "\n",
    "The goal is to get the minimal cost, also known as the vertex (actual value) of the graph. In order to get closer to the minimal cost, receive the derivative (slope) at each point of the current cost to determine the direction (go right if negative, left if positive) to descent the cost.\n",
    "\n",
    "<img src=\"images/ann/gradient_descent_1.png\" height=\"30%\" width=\"30%\"></img>\n",
    "\n",
    "Roll the cost ball to the right because it's a negative derivative.\n",
    "<hr>\n",
    "\n",
    "<img src=\"images/ann/gradient_descent_2.png\" height=\"30%\" width=\"30%\"></img>\n",
    "\n",
    "The cost ball rolled to the right. Now we need to roll the cost ball to the left because it's a positive derivative.\n",
    "<hr>\n",
    "\n",
    "<img src=\"images/ann/gradient_descent_3.png\" height=\"30%\" width=\"30%\"></img>\n",
    "\n",
    "The cost ball rolled to the left. Now we need to roll the cost ball to the right because it's a negative derivative.\n",
    "<hr>\n",
    "\n",
    "<img src=\"images/ann/gradient_descent_4.png\" height=\"30%\" width=\"30%\"></img>\n",
    "\n",
    "The cost ball is now at the minimal cost. We determined the best weights to the neural network!\n",
    "\n",
    "### Partial Derivatives of Gradient Descent\n",
    "What if there were multiple outputs in the neural network aside from just y? How would the cost function work? To handle this, the gradient descent algorithm calculates the partial derivatives of the individual variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "In a convex function like the quadratic Cost function, there is a single minimum. However, if we used a different Cost function that had multiple local minimums, then the Gradient Descent algorithm might not descent to the global minimum but instead to a local minimum.\n",
    "\n",
    "<img src=\"images/ann/gradient_descent_problem.png\" height=\"30%\" width=\"30%\"></img>\n",
    "\n",
    "Notice how the cost ball is at a local minimum, but not the best global minimum. Therefore, we solve this problem by using the Stochastic Gradient Descent!\n",
    "\n",
    "### Comparing The Two Gradient Descents\n",
    "<img src=\"images/ann/batch_vs_stochastic_gradient_descent.png\" height=\"75%\" width=\"75%\"></img>\n",
    "\n",
    "For the Standard Gradient Descent, the algorithm calculates the Cost function by summing the whole \"batch\" (all the rows) of the data set, then it applies the Gradient Descent on the weights. This is an example of Batch Learning!\n",
    "\n",
    "For the Stochastic Gradient Descent, the algorithm calculates the Cost function per each row, then it applies Gradient Descecent on the weights. Basically, it updates the weights one row at a time. This is an example of Reinforcement Learning!\n",
    "\n",
    "### Advantages of Stochastic Gradient Descent\n",
    "1. Finds the global minimum instead of local minimum.  \n",
    "2. It's actually faster because it does not have to load all the data in the memory, much lighter and faster.\n",
    "\n",
    "### Disadvantages of Stochastic Gradient Descent\n",
    "1. The rows may be picked at random, so the neural network is updated at a stochastic (random) manner. Therefore, the number of epochs to minimize the Cost function is also random.\n",
    "\n",
    "### Mini-Batch Gradient Descent\n",
    "Mini-Batch Gradient Descent combines the ideas of Standard and Stochastic Gradient Descent.\n",
    "\n",
    "A batch are the rows (samples) in one forward and back propagation. In Stochastic Gradient Descent the batch size is 1 and in Standard Gradient Descent the batch size is the entire data set.\n",
    "\n",
    "Mini-Batch Gradient Descent specifies the batch size (number of samples) parameter to perform a forward and back propagation, thus combining the ideas of the two algorithms. We will use Mini-Batch Gradient Descent for this lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Algorithm\n",
    "Let's put all the mechanisms together to develop a training algorithm for the artificial neural network.\n",
    "\n",
    "1. Randomly initialize weights to small numbers close to 0.  \n",
    "2. Input the independent variables into the input layer.  \n",
    "3. Perform forward propagation, then measure the Cost function.  \n",
    "4. Perform back propagation, then use the Cost function to update the weights with reinforcement learning (stochastic) or batch learning.\n",
    "5. Redo more epochs, early stop once the validation accuracy flattens out to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the data set, classify if a customer will exit (1) or will not exit (0)\n",
    "churn_df = pd.read_csv(\"datasets/ann/churn_modelling.csv\")\n",
    "\n",
    "churn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x is the columns from column indexes from 3 to 12\n",
    "x = churn_df.iloc[:, 3:13].values\n",
    "\n",
    "# y is the Exited column\n",
    "y = churn_df.iloc[:, 13].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the encoders and column transformer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# label encode the Gender column, no need to one hot encode because it has only 2 categories\n",
    "label_encoder_gender = LabelEncoder()\n",
    "x[:, 2] = label_encoder_gender.fit_transform(x[:, 2])\n",
    "\n",
    "# one hot encode the Geography column\n",
    "gender_column_index = 1\n",
    "transformer = ColumnTransformer(\n",
    "    [(\"one_hot_encoder\", OneHotEncoder(categories=\"auto\"), [gender_column_index])],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "x = transformer.fit_transform(x)\n",
    "\n",
    "# avoid the dummy variable trap from the encoded Geography column\n",
    "x = x[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data set into training and testing data sets\n",
    "from sklearn.model_selection import train_test_split \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a Standarization Scaler for Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# feature scale the training and testing sets\n",
    "sc_x = StandardScaler()\n",
    "x_train = sc_x.fit_transform(x_train)\n",
    "x_test = sc_x.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network Model\n",
    "Now that we pre-processed the data set, let's begin programming the ANN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the sequential model, a standard model to create a neural network\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# import the dense layer, a regularly connected neural network layer\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the neural network as a sequence of layers\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0821 12:09:05.463026 139974163797824 deprecation.py:506] From /home/pravat/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "add the input layer and the first hidden layer\n",
    "- units = 6 means that there are 6 neurons in this hidden layer;\n",
    "    this was determined by a general tip where you get the average of the number\n",
    "    of neurons in the input and output layer, which is (11 + 1) / 2 = 6\n",
    "- kernel_initializer = uniform initializes the weights to small numbers\n",
    "    close to 0 in a uniform distribution\n",
    "- activation = relu stands for the rectifier activation function\n",
    "- input_dim = 11 means that there are 11 neurons (columns or indep. variables) in the input layer\n",
    "\"\"\"\n",
    "classifier.add(\n",
    "    Dense(\n",
    "        units=6,\n",
    "        kernel_initializer=\"uniform\",\n",
    "        activation=\"relu\",\n",
    "        input_dim=11\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the second hidden layer using the same parameters as the first hidden layer\n",
    "classifier.add(\n",
    "    Dense(\n",
    "        units=6,\n",
    "        kernel_initializer=\"uniform\",\n",
    "        activation=\"relu\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "add the output layer using a sigmoid activation function for a single output neuron\n",
    "\n",
    "If you wish to have multiple output neurons, then change the units parameter to the number\n",
    "of output neurons you wish to have and change the activation function to \"softmax\" because\n",
    "the \"sigmoid\" activation function only works best with a single output.\n",
    "\"\"\"\n",
    "classifier.add(\n",
    "    Dense(\n",
    "        units=1,\n",
    "        kernel_initializer=\"uniform\",\n",
    "        activation=\"sigmoid\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0821 12:09:07.025415 139974163797824 deprecation.py:323] From /home/pravat/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "compile the ANN\n",
    "- optimizer = adam is a specific type of Stochastic Gradient Descent algorithm\n",
    "- loss = binary_crossentropy because we're using a sigmoid activation for the output layer\n",
    "- metric = accuracy means to use the accuracy metric to determine how accurate the model is\n",
    "\"\"\"\n",
    "classifier.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 81us/sample - loss: 0.3636 - acc: 0.8536\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 1s 77us/sample - loss: 0.3616 - acc: 0.8534\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 1s 75us/sample - loss: 0.3602 - acc: 0.8530\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 1s 73us/sample - loss: 0.3597 - acc: 0.8529\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 1s 73us/sample - loss: 0.3582 - acc: 0.8545\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 1s 88us/sample - loss: 0.3574 - acc: 0.8565\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 1s 70us/sample - loss: 0.3565 - acc: 0.8570\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 1s 70us/sample - loss: 0.3560 - acc: 0.8530\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 1s 71us/sample - loss: 0.3547 - acc: 0.8575\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 1s 69us/sample - loss: 0.3543 - acc: 0.8575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4dcc52fb70>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "fit the ANN using 10 epochs and update the weights after every 10 rows (batch)\n",
    "\n",
    "The greater the accuracy, the smaller the loss because the loss function is receiving less errors.\n",
    "It's a good idea to early stop training the neural network once the loss begins to increase\n",
    "\"\"\"\n",
    "classifier.fit(x_train, y_train, batch_size = 10, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the testing set\n",
    "y_pred = classifier.predict(x_test)\n",
    "y_pred = y_pred > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the confusion matrix function\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1515,   80],\n",
       "       [ 212,  193]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a confusion matrix that compares the y_test (actual) to the y_pred (prediction)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\"\"\"\n",
    "Read the Confusion Matrix diagonally:\n",
    "1515 + 193 = 1708 correct predictions\n",
    "80 + 212 = 292 incorrect predictions\n",
    "\"\"\"\n",
    "cm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
